{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3574bd",
   "metadata": {},
   "source": [
    "Book: https://fulyankin.github.io/deep_learning_masha_book/problem_set_03_backprop/problem_05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "36495002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0a5de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, dx = 0):\n",
    "    if dx:\n",
    "        y = sigmoid(x, dx = 0)\n",
    "        return y * (1 - y)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def mse_loss(y, y_h):\n",
    "    loss = ((y_h - y) ** 2).mean()\n",
    "    dL_dY = 2 * (y_h - y) / y.shape[0] #d wrt y_h\n",
    "    return loss, dL_dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08320f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.W = np.random.rand(in_dim, out_dim) * 0.1\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return X @ self.W\n",
    "\n",
    "    def backward(self, dY):\n",
    "        self.dW = self.X.T @ dY #d wrt W\n",
    "        return dY @ self.W.T #d wrt X\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, X):\n",
    "        self.Y = sigmoid(X)\n",
    "        return self.Y\n",
    "\n",
    "    def backward(self, dY):\n",
    "        return dY * (self.Y * (1 - self.Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd3fedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self): #Model\n",
    "        self.layers = []\n",
    "        for i in range(2):\n",
    "            self.layers.append(Linear(2, 2))\n",
    "            self.layers.append(Sigmoid())\n",
    "        self.layers.append(Linear(2, 1))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        for i in self.layers:\n",
    "            X = i.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def SGD(self, dL_dY, lr=0.1):\n",
    "        u = len(self.layers) - 1\n",
    "        self.dX = dL_dY #Grad from mse_loss\n",
    "\n",
    "        u = len(self.layers) - 1\n",
    "        while(u >= 0): #Chain rule to find all derrivatives\n",
    "            self.dX = self.layers[u].backward(self.dX)\n",
    "            if(isinstance(self.layers[u], Linear)):\n",
    "                self.layers[u].W -= lr * self.layers[u].dW\n",
    "            u -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5404c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "n = 5 # Number of observations\n",
    "X = np.random.randn(2, n)\n",
    "Y = (X[0] * 0.05 + X[1] * 0.1 + 0.34).reshape(-1, 1)\n",
    "\n",
    "for step in range(2): #Training loop\n",
    "    y_hat = model.predict(X.T)\n",
    "    loss, dL_dY = mse_loss(Y, y_hat) #Grad of loss\n",
    "    model.SGD(dL_dY, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27407c37",
   "metadata": {},
   "source": [
    "Final check, small ε.\n",
    "dL/dw ​≈ ( L(w+ε) − L(w−ε) )​ / 2ε. Error ~ 2e-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "33ba6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28528f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(layer):\n",
    "    i, j = 0, 0\n",
    "\n",
    "    y_hat = model.predict(X.T)\n",
    "    loss, dL_dY = mse_loss(Y, y_hat)\n",
    "    model.SGD(dL_dY, lr=0) #Just grad, don't change weights\n",
    "\n",
    "    grad = layer.dW[i, j]\n",
    "\n",
    "    eps = 1e-5\n",
    "    val = layer.W[i, j]\n",
    "\n",
    "    #L(w + ε)\n",
    "    layer.W[i, j] = val + eps\n",
    "    y_hat_plus = model.predict(X.T)\n",
    "    loss_plus, _ = mse_loss(Y, y_hat_plus)\n",
    "\n",
    "    #L(w - ε)\n",
    "    layer.W[i, j] = val - eps\n",
    "    y_hat_minus = model.predict(X.T)\n",
    "    loss_minus, _ = mse_loss(Y, y_hat_minus)\n",
    "\n",
    "    #Original value\n",
    "    layer.W[i, j] = val\n",
    "\n",
    "    num = (loss_plus - loss_minus) / (2 * eps)\n",
    "    rel_error = abs(grad - num) / max(1.0, abs(grad), abs(num))\n",
    "\n",
    "    print(\"Analytic:\", grad)\n",
    "    print(\"Numeric:\", num)\n",
    "    print(\"Rel. err:\", rel_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e815a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic: -0.29547590450288164\n",
      "Numeric: -0.29547590450276773\n",
      "Rel. err: 1.1390888232654106e-13\n",
      "\n",
      "Analytic: -0.006169263888914528\n",
      "Numeric: -0.00616926388863348\n",
      "Rel. err: 2.8104862187516844e-13\n"
     ]
    }
   ],
   "source": [
    "layer = model.layers[-1] # Linear (2*1)\n",
    "\n",
    "check(layer)\n",
    "print()\n",
    "layer = model.layers[-3]\n",
    "check(layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
